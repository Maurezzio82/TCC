from amatrix_to_env import Environment, State
import numpy as np
import math
import random
import matplotlib
import matplotlib.pyplot as plt
from collections import namedtuple, deque
from itertools import count

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

device = torch.device(
    "cuda" if torch.cuda.is_available() else
    "mps" if torch.backends.mps.is_available() else
    "cpu"
)

class DQN(nn.Module):

    def __init__(self, n_observations, n_actions):
        super(DQN, self).__init__()
        self.layer1 = nn.Linear(n_observations, 128)
        self.layer2 = nn.Linear(128, 128)
        self.layer3 = nn.Linear(128, n_actions)

    # Called with either one element to determine next action, or a batch
    # during optimization. Returns tensor([[left0exp,right0exp]...]).
    def forward(self, x):
        x = F.relu(self.layer1(x))
        x = F.relu(self.layer2(x))
        return self.layer3(x)
    

M = np.array([[0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
              [0, 0, 0, 6, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
              [0, 0, 0, 0, 8, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
              [0, 0, 0, 0, 0, 0, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0],
              [0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 0, 0, 0, 0, 0, 0],
              [0, 0, 0, 0, 0, 0, 0, 0, 6, 5, 0, 0, 0, 0, 0, 0],
              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0],
              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 5, 0, 0, 0, 0],
              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0],
              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0],
              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0],
              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 9, 0],
              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0],
              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3],
              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2],
              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])

env = Environment(M, run_DP = True)

PATH = "Trained_Networks/DQN/Adjencency_Matrix/A_matrix_solution.pth"
policy_net = torch.load(PATH, weights_only=False)

total_reward = 0
terminated = False

state = env.reset()

# Convert state to tensor
state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)

while not terminated:
    with torch.no_grad():  # No gradient needed for testing
        action = policy_net(state).max(1).indices.view(1, 1).item()

    # Take action
    next_state, reward, terminated = env.step(action)

    # Update state
    state = torch.tensor(next_state, dtype=torch.float32, device=device).unsqueeze(0)

    total_reward += reward
    # Check if episode is over
    

terminated = False
env.reset()
env.solve()

current_state = env.current_state
policy = env.optimal_policy
true_total_reward = 0

while not terminated:
    action = int(policy[current_state.index])
    _, reward, terminated = env.step(action)
    current_state = env.current_state
    true_total_reward += reward
 

if true_total_reward >= total_reward:
    print('optimal policy has been found')
else:
    print('optimal policy has not been found')
    


state_list = np.zeros(env.n_states)
V_Qnet = np.zeros(env.n_states)
for i in range(env.n_states):
    state_list[i] = 1
    pytorch_state = torch.tensor(state_list, dtype=torch.float32, device=device).unsqueeze(0)
    V_Qnet[i] = policy_net(pytorch_state).max().item()
    state_list = np.zeros(env.n_states)

diff_norm = np.linalg.norm(env.Valuefunc - V_Qnet)/env.n_states
print(diff_norm)

""" current_state = 
V_Qnet = [] """